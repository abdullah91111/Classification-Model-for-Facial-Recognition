# -*- coding: utf-8 -*-
"""Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KC3aof7zBYntZSqZ4mwAcSWBskHc-x9I
"""

# Import required libraries
import os
import tensorflow as tf
import cv2
import matplotlib.pyplot as plt
import numpy as np
from tensorflow import keras
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam, optimizers
from tensorflow.keras.applications import ResNet50V2
from tensorflow.keras.applications.resnet_v2 import preprocess_input
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom, RandomContrast
import itertools
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Input



TestPath = 'C:/Users/102770295/Desktop/Classification/test_data'
TrainPath = 'C:/Users/102770295/Desktop/Classification/train_data'
ValidationPath = 'C:/Users/102770295/Desktop/Classification/val_data'

def count_directories(path):
    return len([name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))])

# Get the count of directories in each dataset path
train_classes_count = count_directories(TrainPath)
test_classes_count = count_directories(TestPath)
val_classes_count = count_directories(ValidationPath)

# Print the counts
print('total training classes:', train_classes_count)
print('total testing classes:', test_classes_count)
print('total validation classes:', val_classes_count)


# Define the path to your dataset
dataset_path = 'C:/Users/102770295/Desktop/Classification/train_data'

# Prepare to collect image paths and labels
image_paths = []
image_labels = []

# Get all class directories
class_dirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]
class_dirs.sort()  # Sort the class directories for consistency

# Iterate over class directories to get image file names
for class_label, class_dir in enumerate(class_dirs):
    # Full path to the class directory
    class_dir_path = os.path.join(dataset_path, class_dir)

    # List all files in the class directory
    image_files = os.listdir(class_dir_path)

    for image_file in image_files:
        # Full path to the image file
        image_file_path = os.path.join(class_dir_path, image_file)
        # Check if it's a file and not a subdirectory
        if os.path.isfile(image_file_path):
            # Append the image path and label to the respective lists
            image_paths.append(image_file_path)
            image_labels.append(class_label)

# Open a new labels_train.txt file in write mode
with open('labels_train.txt', 'w') as f:
    for path, label in zip(image_paths, image_labels):
        # Write the image path and its corresponding label to the file
        f.write(f"{path} {label}\n")

print("labels_train.txt with image paths and labels has been created")

import os

# Define the path to your dataset
dataset_path = 'C:/Users/102770295/Desktop/Classification/test_data'

# Prepare to collect image paths and labels
image_paths = []
image_labels = []

# Get all class directories
class_dirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]
class_dirs.sort()  # Sort the class directories for consistency

# Iterate over class directories to get image file names
for class_label, class_dir in enumerate(class_dirs):
    # Full path to the class directory
    class_dir_path = os.path.join(dataset_path, class_dir)

    # List all files in the class directory
    image_files = os.listdir(class_dir_path)

    for image_file in image_files:
        # Full path to the image file
        image_file_path = os.path.join(class_dir_path, image_file)
        # Check if it's a file and not a subdirectory
        if os.path.isfile(image_file_path):
            # Append the image path and label to the respective lists
            image_paths.append(image_file_path)
            image_labels.append(class_label)

# Open a new labels_test.txt file in write mode
with open('labels_test.txt', 'w') as f:
    for path, label in zip(image_paths, image_labels):
        # Write the image path and its corresponding label to the file
        f.write(f"{path} {label}\n")

print("labels_test.txt with image paths and labels has been created")

import os

# Define the path to your dataset
dataset_path = 'C:/Users/102770295/Desktop/Classification/val_data'

# Prepare to collect image paths and labels
image_paths = []
image_labels = []

# Get all class directories
class_dirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]
class_dirs.sort()  # Sort the class directories for consistency

# Iterate over class directories to get image file names
for class_label, class_dir in enumerate(class_dirs):
    # Full path to the class directory
    class_dir_path = os.path.join(dataset_path, class_dir)

    # List all files in the class directory
    image_files = os.listdir(class_dir_path)

    for image_file in image_files:
        # Full path to the image file
        image_file_path = os.path.join(class_dir_path, image_file)
        # Check if it's a file and not a subdirectory
        if os.path.isfile(image_file_path):
            # Append the image path and label to the respective lists
            image_paths.append(image_file_path)
            image_labels.append(class_label)

# Open a new labels_val.txt file in write mode
with open('labels_val.txt', 'w') as f:
    for path, label in zip(image_paths, image_labels):
        # Write the image path and its corresponding label to the file
        f.write(f"{path} {label}\n")

print("labels_val.txt with image paths and labels has been created")


#link names from text file to path
train_info = 'C:/Users/102770295/Desktop/Classification/labels_train.txt'
with open(train_info) as txt_file:
    lines =  [x.strip() for x in txt_file.readlines()]
x_train_path = [x.split(' ')[0] for x in lines]
y_train = [x.split(' ')[1] for x in lines]

test_info = 'C:/Users/102770295/Desktop/Classification/labels_test.txt'
with open(test_info) as txt_file:
    lines =  [x.strip() for x in txt_file.readlines()]
x_test_path = [x.split(' ')[0] for x in lines]
y_test = [x.split(' ')[1] for x in lines]

val_info = 'C:/Users/102770295/Desktop/Classification/labels_val.txt'
with open(val_info) as txt_file:
    lines =  [x.strip() for x in txt_file.readlines()]
x_val_path = [x.split(' ')[0] for x in lines]
y_val = [x.split(' ')[1] for x in lines]

#data generator for loading data to model


#The class is inherit the properties of keras.utils.Sequence so that we can leverage nice functionalities such as multiprocessing.

class DataGenerator(keras.utils.Sequence):
    'Generates data for Keras'
    def _init_(self, list_IDs, labels, batch_size=32, dim=(224,224), n_channels=3,
                 n_classes=4000, shuffle=True):
        'Initialization'
        self.dim = dim
        self.batch_size = batch_size
        self.labels = labels
        self.list_IDs = list_IDs
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.on_epoch_end()

    def _len_(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.list_IDs) / self.batch_size))

    def _getitem_(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        list_IDs_temp = [self.list_IDs[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(list_IDs_temp)

        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples'
        # Initialization
        X = np.empty((self.batch_size, *self.dim, self.n_channels))
        y = np.empty((self.batch_size), dtype=int)

        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            # Load and preprocess the image
            img = self.load_and_preprocess_image(ID, self.dim)
            X[i,] = img

            # Store class
            y[i] = self.labels[ID]

        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)


    def load_and_preprocess_image(self, image_path, target_size):
        # Load the image file
        image = cv2.imread(image_path)
        image = cv2.resize(image, target_size)  # Resize image
        # No need to convert to RGB as preprocess_input will handle it
        image = preprocess_input(image)  # Use ResNet50V2's preprocess_input
        return image

#set up generators

def load_labels(file_path, base_img_path):
    list_IDs = []
    labels = {}
    with open(file_path, 'r') as file:
        for line in file:
            #format: 'n00003/img1.jpg label'
            # Split the line into image path and label using rsplit
            relative_image_path, label = line.strip().rsplit(' ', 1)
            # Clean up the label by stripping any whitespace or newlines
            label = label.strip()
            # Construct the full path to the image
            full_image_path = os.path.join(base_img_path, relative_image_path)
            list_IDs.append(full_image_path)
            labels[full_image_path] = int(label)  # Convert label to integer if necessary
    return list_IDs, labels

# The base path where the 'train_data' directory is located
base_img_path = 'C:/Users/102770295/Desktop/Classification/train_data'

# The path to your 'labels_train.txt' file
labels_file_path = 'C:/Users/102770295/Desktop/Classification/labels_train.txt'

# Load the list of image IDs and the corresponding labels
list_IDs, labels = load_labels(labels_file_path, base_img_path)

# Initialize your DataGenerator with these lists
training_generator = DataGenerator(list_IDs, labels)

#set up generators

def load_labels(file_path, base_img_path):
    list_IDs = []
    labels = {}
    with open(file_path, 'r') as file:
        for line in file:
            #format: 'n00003/img1.jpg label'
            # Split the line into image path and label using rsplit
            relative_image_path, label = line.strip().rsplit(' ', 1)
            # Clean up the label by stripping any whitespace or newlines
            label = label.strip()
            # Construct the full path to the image
            full_image_path = os.path.join(base_img_path, relative_image_path)
            list_IDs.append(full_image_path)
            labels[full_image_path] = int(label)  # Convert label to integer if necessary
    return list_IDs, labels

# The base path where the 'test_data' directory is located
base_test_img_path = 'C:/Users/102770295/Desktop/Classification/test_data'

# The path to your 'test_train.txt' file
labels_test_file_path = 'C:/Users/102770295/Desktop/Classification/labels_test.txt'

# Load the list of image IDs and the corresponding labels
list_IDs_test, labels_test = load_labels(labels_test_file_path, base_test_img_path)

# Initialize your DataGenerator with these lists
testing_generator = DataGenerator(list_IDs_test, labels_test)

def load_labels(file_path, base_img_path):
    list_IDs = []
    labels = {}
    with open(file_path, 'r') as file:
        for line in file:
            #n00003/img1.jpg label'
           # Split the line into image path and label using rsplit
            relative_image_path, label = line.strip().rsplit(' ', 1)
            # Clean up the label by stripping any whitespace or newlines
            label = label.strip()
            # Construct the full path to the image
            full_image_path = os.path.join(base_img_path, relative_image_path)
            list_IDs.append(full_image_path)
            labels[full_image_path] = int(label)
    return list_IDs, labels

# 'validation_data' directory
base_validation_img_path = 'C:/Users/102770295/Desktop/Classification/val_data'

#path to your 'labels_val.txt' file
labels_val_file_path = 'C:/Users/102770295/Desktop/Classification/labels_val.txt'

# Load the list of validation image IDs and the corresponding labels
list_IDs_val, labels_val = load_labels(labels_val_file_path, base_validation_img_path)

# Instantiate the validation DataGenerator
validation_generator = DataGenerator(list_IDs_val, labels_val)

#training_generator and validation_generator are previously defined

#Model ResNet50V2

# Define the number of classes in your dataset
num_classes = 4000

# Data Augmentation
data_augmentation = tf.keras.Sequential([
    RandomFlip("horizontal"),
    RandomZoom(0.1),  # Zooms by 10%
    RandomContrast(0.1),  # Adjusts contrast by ±10%
])

# Load pre-trained ResNet50V2 model (excluding the top layer)
base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the first layers of the pre-trained model
for layer in base_model.layers:
    layer.trainable = False

# Create a custom head for the network
inputs = Input(shape=(224, 224, 3))
x = data_augmentation(inputs)
x = base_model(x, training=False)
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(num_classes, activation='softmax')(x)

# Combine the base model with the new head
model = Model(inputs=inputs, outputs=predictions)

# Define metrics
precision_metric = Precision(name='precisio01n')
recall_metric = Recall(name='recall')

# Compile the model
model.compile(optimizer=Adam(lr=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy', precision_metric, recall_metric])

# Setup ModelCheckpoint callback
checkpoint_callback = ModelCheckpoint(
    filepath="model_epoch_{epoch:02d}.h5",
    save_weights_only=True,
    save_freq='epoch'
)

early_stopping_callback = EarlyStopping(
    monitor='val_loss',
    patience=5,  # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity
)


# Train the model with both training and validation data
history = model.fit(
    training_generator,
    steps_per_epoch=len(training_generator),
    epochs=30,
    validation_data=validation_generator,
    validation_steps=len(validation_generator),
    verbose=1,
    callbacks=[checkpoint_callback, early_stopping_callback]
)

if early_stopping_callback.stopped_epoch > 0:
    print(f"Early stopping triggered at epoch {early_stopping_callback.stopped_epoch}")

# Unfreeze the top layers of the model
for layer in base_model.layers:
    layer.trainable = True

# Re-compile the model with a lower learning rate for fine-tuning
model.compile(optimizer=optimizers.Adam(lr=0.000001),
              loss='categorical_crossentropy',
              metrics=['accuracy', precision_metric, recall_metric])

# Fine-tuning training
fine_tune_epochs = 30
total_epochs = 30 + fine_tune_epochs

history_fine = model.fit(
    training_generator,
    steps_per_epoch=len(training_generator),
    epochs=total_epochs,
    initial_epoch=history.epoch[-1],
    validation_data=validation_generator,
    validation_steps=len(validation_generator),
    verbose=1,
    callbacks=[checkpoint_callback, early_stopping_callback]
)

if early_stopping_callback.stopped_epoch > 0:
    print(f"Early stopping triggered at epoch {early_stopping_callback.stopped_epoch}")

# Evaluate the model on the validation set
evaluation_results = model.evaluate(
    validation_generator,
    verbose=1
)
print(f"Validation Loss: {evaluation_results[0]}, Validation Accuracy: {evaluation_results[1]}")

# Make predictions on the validation set
predictions = model.predict(
    testing_generator,
    verbose=1
)

predicted_classes = np.argmax(predictions, axis=1)

# Save the final model weights to a file
model.save_weights("final_model_weights.h5")

# Save the entire model to a file
model.save("final_model.h5")

# Print the model summary
model.summary()

# Training and validation accuracies
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_acc_percentage = [acc * 100 for acc in train_acc]
val_acc_percentage = [acc * 100 for acc in val_acc]

# After training, you can access these metrics from the history object
train_precision = history.history['precision']
train_recall = history.history['recall']

# For validation metrics
val_precision = history.history['val_precision']
val_recall = history.history['val_recall']

# Print out the final training and validation accuracies
print(f"Final Training Accuracy: {train_acc_percentage[-1]:.2f}%")
print(f"Final Validation Accuracy: {val_acc_percentage[-1]:.2f}%")

# Plotting the accuracies
plt.figure(figsize=(10, 5))
plt.plot(train_acc_percentage, label='Training Accuracy')
plt.plot(val_acc_percentage, label='Validation Accuracy')
plt.title('Training and Validation Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.show()

# Fine-tuning training and validation accuracies
fine_tune_train_acc = history_fine.history['accuracy']
fine_tune_val_acc = history_fine.history['val_accuracy']
fine_tune_train_acc_percentage = [acc * 100 for acc in fine_tune_train_acc]
fine_tune_val_acc_percentage = [acc * 100 for acc in fine_tune_val_acc]


# After training, you can access these metrics from the history object
train_precision = history.history['precision']
train_recall = history.history['recall']


# For validation metrics
val_precision = history_fine.history['val_precision']
val_recall = history_fine.history['val_recall']

# Print out the final training and validation accuracies for fine-tuned model
print(f"Final Fine-Tuned Training Accuracy: {fine_tune_train_acc_percentage[-1]:.2f}%")
print(f"Final Fine-Tuned Validation Accuracy: {fine_tune_val_acc_percentage[-1]:.2f}%")
# Print training metrics
print("Training Metrics:")
print(f"Precision: {train_precision[-1]}")
print(f"Recall: {train_recall[-1]}")

# Print validation metrics
print("\nValidation Metrics:")
print(f"Precision: {val_precision[-1]}")
print(f"Recall: {val_recall[-1]}")

# Plotting the accuracies for fine-tuned model
plt.figure(figsize=(10, 5))
plt.plot(fine_tune_train_acc_percentage, label='Training Accuracy')
plt.plot(fine_tune_val_acc_percentage, label='Validation Accuracy')
plt.title('Training and Validation Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.show()